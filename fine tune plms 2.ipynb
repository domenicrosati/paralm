{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a1f5553",
   "metadata": {},
   "outputs": [],
   "source": [
    "!git clone https://github.com/dqxiu/ParaSCI.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0740ff1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./ParaSCI/Data/ParaSCI-ACL/train/train.src') as file:\n",
    "    train_source = file.readlines()\n",
    "\n",
    "with open('./ParaSCI/Data/ParaSCI-ACL/train/train.tgt') as file:\n",
    "    train_target = file.readlines()\n",
    "    \n",
    "with open('./ParaSCI/Data/ParaSCI-ACL/test/test.src') as file:\n",
    "    test_source = file.readlines()\n",
    "\n",
    "with open('./ParaSCI/Data/ParaSCI-ACL/test/test.tgt') as file:\n",
    "    test_target = file.readlines()\n",
    "    \n",
    "with open('./ParaSCI/Data/ParaSCI-ACL/val/val.src') as file:\n",
    "    val_source = file.readlines()\n",
    "\n",
    "with open('./ParaSCI/Data/ParaSCI-ACL/val/val.tgt') as file:\n",
    "    val_target = file.readlines()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5b1ea66b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset, DatasetDict, load_metric\n",
    "import numpy as np\n",
    "\n",
    "train_dataset = Dataset.from_dict({\n",
    "    'id': np.arange(len(train_source)),\n",
    "    'paraphrase': [{'input': data[0], 'output': data[1]} for data in zip(train_source, train_target)],\n",
    "})\n",
    "test_dataset = Dataset.from_dict({\n",
    "        'id': np.arange(len(test_source)),\n",
    "    'paraphrase': [{'input': data[0], 'output': data[1]} for data in zip(test_source, test_target)],\n",
    "})\n",
    "val_dataset = Dataset.from_dict({\n",
    "    'id': np.arange(len(val_source)),\n",
    "    'paraphrase': [{'input': data[0], 'output': data[1]} for data in zip(val_source, val_target)]\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "406d2da9",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_dataset = DatasetDict()\n",
    "raw_dataset['train'] = train_dataset\n",
    "raw_dataset['test'] = test_dataset\n",
    "raw_dataset['val'] = val_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fe865287",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['id', 'paraphrase'],\n",
       "        num_rows: 28883\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['id', 'paraphrase'],\n",
       "        num_rows: 2345\n",
       "    })\n",
       "    val: Dataset({\n",
       "        features: ['id', 'paraphrase'],\n",
       "        num_rows: 2753\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "00a480f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/domenicrosati/.pyenv/versions/3.9.0/lib/python3.9/site-packages/transformers/pipelines/__init__.py:652: UserWarning: \"translation\" task was used, instead of \"translation_XX_to_YY\", defaulting to \"translation_en_to_de\"\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'translation_text': 'Standard f√ºr erweiterte Threads'}]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "model_checkpoint = \"t5-small\"\n",
    "translator = pipeline(\"translation\", model=model_checkpoint)\n",
    "translator(\"Default to expanded threads\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "80bbfd07",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint, return_tensors=\"tf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e40526c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_input_length = 128\n",
    "max_target_length = 128\n",
    "\n",
    "\n",
    "def preprocess_function(examples):\n",
    "    inputs = [ex[\"input\"] for ex in examples[\"paraphrase\"]]\n",
    "    targets = [ex[\"output\"] for ex in examples[\"paraphrase\"]]\n",
    "    model_inputs = tokenizer(inputs, max_length=max_input_length, truncation=True)\n",
    "\n",
    "    # Set up the tokenizer for targets\n",
    "    with tokenizer.as_target_tokenizer():\n",
    "        labels = tokenizer(targets, max_length=max_target_length, truncation=True)\n",
    "\n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    return model_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d166ebc7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "224e19ae43404aefb7f9859b52cfab4f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/29 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6534c1cbbbd949cb94ac534014837e4a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "611b48ed6aec4462b3ab2e5adf96b0d0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenized_datasets = raw_dataset.map(\n",
    "    preprocess_function,\n",
    "    batched=True,\n",
    "    remove_columns=raw_dataset[\"train\"].column_names,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "365b1381",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSeq2SeqLM\n",
    "\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "112afceb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: sacrebleu in /Users/domenicrosati/.pyenv/versions/3.9.0/lib/python3.9/site-packages (2.0.0)\r\n",
      "Requirement already satisfied: numpy>=1.17 in /Users/domenicrosati/.pyenv/versions/3.9.0/lib/python3.9/site-packages (from sacrebleu) (1.20.0)\r\n",
      "Requirement already satisfied: portalocker in /Users/domenicrosati/.pyenv/versions/3.9.0/lib/python3.9/site-packages (from sacrebleu) (2.4.0)\r\n",
      "Requirement already satisfied: colorama in /Users/domenicrosati/.pyenv/versions/3.9.0/lib/python3.9/site-packages (from sacrebleu) (0.4.4)\r\n",
      "Requirement already satisfied: regex in /Users/domenicrosati/.pyenv/versions/3.9.0/lib/python3.9/site-packages (from sacrebleu) (2021.3.17)\r\n",
      "Requirement already satisfied: tabulate>=0.8.9 in /Users/domenicrosati/.pyenv/versions/3.9.0/lib/python3.9/site-packages (from sacrebleu) (0.8.9)\r\n"
     ]
    }
   ],
   "source": [
    "!pip install sacrebleu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "825e9d43",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DataCollatorForSeq2Seq\n",
    "\n",
    "data_collator = DataCollatorForSeq2Seq(tokenizer, model=model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "eae96d57",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_metric\n",
    "\n",
    "metric = load_metric(\"sacrebleu\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9732a42c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "def compute_metrics(eval_preds):\n",
    "    preds, labels = eval_preds\n",
    "    # In case the model returns more than the prediction logits\n",
    "    if isinstance(preds, tuple):\n",
    "        preds = preds[0]\n",
    "\n",
    "    decoded_preds = tokenizer.batch_decode(preds, skip_special_tokens=True)\n",
    "\n",
    "    # Replace -100s in the labels as we can't decode them\n",
    "    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
    "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "\n",
    "    # Some simple post-processing\n",
    "    decoded_preds = [pred.strip() for pred in decoded_preds]\n",
    "    decoded_labels = [[label.strip()] for label in decoded_labels]\n",
    "\n",
    "    result = metric.compute(predictions=decoded_preds, references=decoded_labels)\n",
    "    return {\"bleu\": result[\"score\"]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "71249d24",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Seq2SeqTrainingArguments\n",
    "\n",
    "args = Seq2SeqTrainingArguments(\n",
    "    f\"t5-finetuned-parasci\",\n",
    "    evaluation_strategy=\"no\",\n",
    "    save_strategy=\"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=32,\n",
    "    per_device_eval_batch_size=64,\n",
    "    weight_decay=0.01,\n",
    "    save_total_limit=3,\n",
    "    num_train_epochs=3,\n",
    "    predict_with_generate=True,\n",
    "    push_to_hub=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "06736a97",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/domenicrosati/src/paralm/t5-finetuned-parasci is already a clone of https://huggingface.co/domenicrosati/t5-finetuned-parasci. Make sure you pull the latest changes with `repo.git_pull()`.\n"
     ]
    }
   ],
   "source": [
    "from transformers import Seq2SeqTrainer\n",
    "\n",
    "trainer = Seq2SeqTrainer(\n",
    "    model,\n",
    "    args,\n",
    "    train_dataset=tokenized_datasets[\"train\"],\n",
    "    eval_dataset=tokenized_datasets[\"val\"],\n",
    "    data_collator=data_collator,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f4f13e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 2753\n",
      "  Batch size = 64\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='36' max='44' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [36/44 15:34 < 03:33, 0.04 it/s]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "trainer.evaluate(max_length=max_target_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6fcf6d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcabf515",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.evaluate(max_length=max_target_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52d8beb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.push_to_hub(tags=\"translation\", commit_message=\"Training complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "787c9a91",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "# Replace this with your own checkpoint\n",
    "model_checkpoint = \"domenicrosati/t5-finetuned-parasci\"\n",
    "translator = pipeline(\"translation\", model=model_checkpoint)\n",
    "translator(\"Default to expanded threads\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02b78f5c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
